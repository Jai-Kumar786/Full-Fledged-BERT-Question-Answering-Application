{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPfic1TMOK/yrS7ybdnZ0Ez",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jai-Kumar786/Full-Fledged-BERT-Question-Answering-Application/blob/main/04_interactive_qa_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook 4: Interactive Q&A System\n",
        "## BERT Question Answering Project\n",
        "\n",
        "**Objectives:** Load fine-tuned model, create interactive Q&A interface\n",
        "**Final Deliverable:** Production-ready question answering system\n"
      ],
      "metadata": {
        "id": "srPnbDsv8LTE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6bToeerw7txy",
        "outputId": "03b0ffbd-0028-424a-a6c1-57835f899b07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Loading fine-tuned BERT model...\n",
            "✅ Model loaded: BertForQuestionAnswering\n",
            "✅ Tokenizer loaded: BertTokenizerFast\n",
            "✅ Model parameters: 108,893,186\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# LOAD FINE-TUNED MODEL & TOKENIZER\n",
        "# ============================================\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
        "import torch\n",
        "\n",
        "print(\"📦 Loading fine-tuned BERT model...\")\n",
        "\n",
        "# Load tokenizer and model from saved directory\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./bert-qa-model\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"./bert-qa-model\")\n",
        "\n",
        "print(f\"✅ Model loaded: {model.__class__.__name__}\")\n",
        "print(f\"✅ Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
        "print(f\"✅ Model parameters: {model.num_parameters():,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Question Answering Pipeline ​\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Action: Use Hugging Face pipeline for easy inference\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "97Za356hAJ6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CREATE QA PIPELINE\n",
        "# ============================================\n",
        "\n",
        "# Create question-answering pipeline\n",
        "qa_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1  # GPU if available\n",
        ")\n",
        "\n",
        "print(\"✅ QA Pipeline created!\")\n",
        "print(f\"✅ Running on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YERmFXJyAQdP",
        "outputId": "38e3c6bd-a91c-4fc5-8106-a9119350f08a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ QA Pipeline created!\n",
            "✅ Running on: GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# TEST THE PIPELINE\n",
        "# ============================================\n",
        "\n",
        "# Test with a simple example\n",
        "test_context = \"\"\"\n",
        "Paris is the capital and most populous city of France.\n",
        "The city has a population of 2.2 million people.\n",
        "Paris is known for the Eiffel Tower and the Louvre Museum.\n",
        "\"\"\"\n",
        "\n",
        "test_question = \"What is the capital of France?\"\n",
        "\n",
        "# Get answer\n",
        "result = qa_pipeline(question=test_question, context=test_context)\n",
        "\n",
        "print(\"\\n🔍 TEST QUERY:\")\n",
        "print(f\"Question: {test_question}\")\n",
        "print(f\"Answer: {result['answer']}\")\n",
        "print(f\"Confidence: {result['score']:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tbW3eYlHArxl",
        "outputId": "933bf0c3-d63f-4ca3-8555-44765d5cab31"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 TEST QUERY:\n",
            "Question: What is the capital of France?\n",
            "Answer: Paris\n",
            "Confidence: 67.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4.2: Build interactive Q&A system (3 marks)"
      ],
      "metadata": {
        "id": "WPLsr1_XA8NZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# QUESTION 4.2: Interactive Q&A System (3 marks)\n",
        "# ============================================\n",
        "\n",
        "def interactive_qa():\n",
        "    \"\"\"\n",
        "    Interactive Q&A system using fine-tuned BERT model.\n",
        "    User provides context and question, system returns answer.\n",
        "    Type 'quit' to exit.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"🤖 BERT Question Answering System\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Instructions:\")\n",
        "    print(\"  1. Enter a context (paragraph of text)\")\n",
        "    print(\"  2. Enter a question about the context\")\n",
        "    print(\"  3. Get an answer from BERT!\")\n",
        "    print(\"  - Type 'quit' at any prompt to exit\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    while True:\n",
        "        # Get context from user\n",
        "        print(\"\\n📝 Enter Context:\")\n",
        "        context = input(\"> \")\n",
        "\n",
        "        # Check for quit\n",
        "        if context.lower() == 'quit':\n",
        "            print(\"\\n👋 Thanks for using BERT QA! Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Validate context\n",
        "        if len(context.strip()) < 10:\n",
        "            print(\"⚠️ Context too short! Please provide at least 10 characters.\")\n",
        "            continue\n",
        "\n",
        "        # Get question from user\n",
        "        print(\"\\n❓ Enter Question:\")\n",
        "        question = input(\"> \")\n",
        "\n",
        "        # Check for quit\n",
        "        if question.lower() == 'quit':\n",
        "            print(\"\\n👋 Thanks for using BERT QA! Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Validate question\n",
        "        if len(question.strip()) < 3:\n",
        "            print(\"⚠️ Question too short! Please ask a proper question.\")\n",
        "            continue\n",
        "\n",
        "        # Get answer from model\n",
        "        print(\"\\n🔍 Processing...\")\n",
        "        try:\n",
        "            result = qa_pipeline(question=question, context=context)\n",
        "\n",
        "            # Display results\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"✅ ANSWER:\")\n",
        "            print(f\"   {result['answer']}\")\n",
        "            print(f\"\\n📊 Confidence: {result['score']:.2%}\")\n",
        "            print(f\"📍 Position: characters {result['start']}-{result['end']}\")\n",
        "            print(\"=\"*70)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ Error: {str(e)}\")\n",
        "            print(\"Please try again with different input.\")\n",
        "\n",
        "# Start the interactive system\n",
        "interactive_qa()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "SR_ElsnqA_QZ",
        "outputId": "df01cce0-ac88-4994-aeb1-1433678ba923"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "🤖 BERT Question Answering System\n",
            "======================================================================\n",
            "Instructions:\n",
            "  1. Enter a context (paragraph of text)\n",
            "  2. Enter a question about the context\n",
            "  3. Get an answer from BERT!\n",
            "  - Type 'quit' at any prompt to exit\n",
            "======================================================================\n",
            "\n",
            "\n",
            "📝 Enter Context:\n",
            "> The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel.\n",
            "\n",
            "❓ Enter Question:\n",
            "> Who is the Eiffel Tower named after?\n",
            "\n",
            "🔍 Processing...\n",
            "\n",
            "======================================================================\n",
            "✅ ANSWER:\n",
            "   Gustave Eiffel\n",
            "\n",
            "📊 Confidence: 76.41%\n",
            "📍 Position: characters 119-133\n",
            "======================================================================\n",
            "\n",
            "📝 Enter Context:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4088184330.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# Start the interactive system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0minteractive_qa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4088184330.py\u001b[0m in \u001b[0;36minteractive_qa\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Get context from user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n📝 Enter Context:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"> \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Check for quit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xgybHN56DkB_",
        "outputId": "a5d06d56-0d49-445d-9298-f5509d311bb7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.50.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.9.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.10.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.50.0-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.50.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Create your Streamlit app file\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# Page config\n",
        "st.set_page_config(\n",
        "    page_title=\"BERT Q&A System\",\n",
        "    page_icon=\"🤖\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# Title\n",
        "st.title(\"🤖 BERT Question Answering System\")\n",
        "st.markdown(\"*Fine-tuned on SQuAD dataset - by Jai Kumar*\")\n",
        "\n",
        "# Load model (cached)\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"./bert-qa-model\")\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(\"./bert-qa-model\")\n",
        "    qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
        "    return qa_pipeline\n",
        "\n",
        "with st.spinner(\"🔄 Loading BERT model...\"):\n",
        "    qa_pipeline = load_model()\n",
        "\n",
        "st.success(\"✅ Model loaded successfully!\")\n",
        "\n",
        "# Sidebar\n",
        "with st.sidebar:\n",
        "    st.header(\"ℹ️ About\")\n",
        "    st.markdown(\"\"\"\n",
        "    This system uses **BERT fine-tuned on SQuAD** to answer questions.\n",
        "\n",
        "    **How to use:**\n",
        "    1. Enter context (paragraph)\n",
        "    2. Ask a question\n",
        "    3. Get AI answer!\n",
        "    \"\"\")\n",
        "\n",
        "    st.header(\"📊 Model Info\")\n",
        "    st.metric(\"Parameters\", \"109M\")\n",
        "    st.metric(\"Training Data\", \"3,000 examples\")\n",
        "    st.metric(\"Validation Loss\", \"1.65\")\n",
        "\n",
        "# Main interface\n",
        "context = st.text_area(\n",
        "    \"📝 Context\",\n",
        "    height=200,\n",
        "    placeholder=\"Enter a paragraph of text here...\",\n",
        "    help=\"Provide the context from which BERT will extract answers\"\n",
        ")\n",
        "\n",
        "question = st.text_input(\n",
        "    \"❓ Question\",\n",
        "    placeholder=\"Ask a question about the context...\",\n",
        "    help=\"Ask a specific question about the context above\"\n",
        ")\n",
        "\n",
        "col1, col2 = st.columns([1, 4])\n",
        "with col1:\n",
        "    ask_button = st.button(\"🔍 Get Answer\", type=\"primary\", use_container_width=True)\n",
        "with col2:\n",
        "    if st.button(\"🔄 Clear\", use_container_width=True):\n",
        "        st.rerun()\n",
        "\n",
        "if ask_button:\n",
        "    if context and question:\n",
        "        with st.spinner(\"🤔 BERT is thinking...\"):\n",
        "            try:\n",
        "                result = qa_pipeline(question=question, context=context)\n",
        "\n",
        "                st.success(\"✅ Answer Found!\")\n",
        "\n",
        "                # Display answer\n",
        "                st.markdown(\"### 💡 Answer:\")\n",
        "                st.markdown(f\"## {result['answer']}\")\n",
        "\n",
        "                # Metrics\n",
        "                col1, col2, col3 = st.columns(3)\n",
        "                with col1:\n",
        "                    st.metric(\"Confidence\", f\"{result['score']:.2%}\")\n",
        "                with col2:\n",
        "                    st.metric(\"Start Position\", result['start'])\n",
        "                with col3:\n",
        "                    st.metric(\"End Position\", result['end'])\n",
        "\n",
        "                # Show answer in context\n",
        "                with st.expander(\"📍 See answer highlighted in context\"):\n",
        "                    before = context[:result['start']]\n",
        "                    answer = context[result['start']:result['end']]\n",
        "                    after = context[result['end']:]\n",
        "                    st.markdown(f\"{before}**:green[{answer}]**{after}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"❌ Error: {str(e)}\")\n",
        "    else:\n",
        "        st.warning(\"⚠️ Please provide both context and question!\")\n",
        "\n",
        "# Example section\n",
        "with st.expander(\"📚 Try Example Questions\"):\n",
        "    example = st.selectbox(\n",
        "        \"Choose an example:\",\n",
        "        [\"\", \"Example 1: Paris\", \"Example 2: Eiffel Tower\", \"Example 3: Machine Learning\"]\n",
        "    )\n",
        "\n",
        "    if example == \"Example 1: Paris\":\n",
        "        st.code(\"\"\"\n",
        "Context: Paris is the capital and most populous city of France. It has an area of 105 square kilometres and a population of 2.2 million people.\n",
        "\n",
        "Question: What is the capital of France?\n",
        "Expected Answer: Paris\n",
        "        \"\"\")\n",
        "    elif example == \"Example 2: Eiffel Tower\":\n",
        "        st.code(\"\"\"\n",
        "Context: The Eiffel Tower was built by Gustave Eiffel in 1889. It is 330 meters tall and located in Paris, France.\n",
        "\n",
        "Question: When was the Eiffel Tower built?\n",
        "Expected Answer: 1889\n",
        "        \"\"\")\n",
        "    elif example == \"Example 3: Machine Learning\":\n",
        "        st.code(\"\"\"\n",
        "Context: BERT stands for Bidirectional Encoder Representations from Transformers. It was developed by Google and published in 2018.\n",
        "\n",
        "Question: Who developed BERT?\n",
        "Expected Answer: Google\n",
        "        \"\"\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wW28x1I_DfX-",
        "outputId": "be483946-7acc-49a6-b292-48a6d8675826"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VIyHvAvHFQdd",
        "outputId": "e74cb966-f3de-4ff8-afac-ac3d14440382"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 831ms\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt &\n"
      ],
      "metadata": {
        "id": "p8yf6G6zFVA6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tpBhkzIjFZvT",
        "outputId": "c867ead2-75b4-430c-fa30-ccbfc6ff8114"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://big-taxis-warn.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://loca.lt/mytunnelpassword\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Zzqss4_qGzWh",
        "outputId": "32665d23-701c-4c11-e03b-b23263994276"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.126.89.157"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# EASIEST SOLUTION: USE GRADIO (NO PASSWORD!)\n",
        "# ============================================\n",
        "\n",
        "# Step 1: Install\n",
        "!pip install -q gradio transformers torch\n",
        "\n",
        "# Step 2: Create Gradio app\n",
        "import gradio as gr\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "# Load model\n",
        "print(\"Loading model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./bert-qa-model\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"./bert-qa-model\")\n",
        "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
        "print(\"✅ Model loaded!\")\n",
        "\n",
        "# Define function\n",
        "def answer_question(context, question):\n",
        "    if not context or not question:\n",
        "        return \"⚠️ Please provide both context and question!\", \"N/A\", \"N/A\"\n",
        "\n",
        "    try:\n",
        "        result = qa_pipeline(question=question, context=context)\n",
        "        return result['answer'], f\"{result['score']:.2%}\", f\"{result['start']}-{result['end']}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\", \"N/A\", \"N/A\"\n",
        "\n",
        "# Create Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=answer_question,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=10, label=\"📝 Context\", placeholder=\"Enter a paragraph...\"),\n",
        "        gr.Textbox(lines=2, label=\"❓ Question\", placeholder=\"Ask a question...\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"✅ Answer\"),\n",
        "        gr.Textbox(label=\"📊 Confidence\"),\n",
        "        gr.Textbox(label=\"📍 Position\")\n",
        "    ],\n",
        "    title=\"🤖 BERT Question Answering System\",\n",
        "    description=\"Fine-tuned BERT on SQuAD dataset - Deep Learning Assignment by Jai Kumar\",\n",
        "    examples=[\n",
        "        [\"Paris is the capital of France. It has 2.2 million people.\", \"What is the capital of France?\"],\n",
        "        [\"The Eiffel Tower was built by Gustave Eiffel in 1889.\", \"When was the Eiffel Tower built?\"],\n",
        "        [\"BERT was developed by Google and published in 2018.\", \"Who developed BERT?\"]\n",
        "    ],\n",
        "    theme=gr.themes.Soft(),\n",
        "    flagging_mode=\"never\"\n",
        ")\n",
        "\n",
        "# Launch with PUBLIC LINK (NO PASSWORD NEEDED!)\n",
        "interface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "-EV1-QqPJb1v",
        "outputId": "e2e7a8aa-2c56-4d35-8482-106decc40c9f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n",
            "✅ Model loaded!\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://9997d8ab4f69f1e40a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9997d8ab4f69f1e40a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}