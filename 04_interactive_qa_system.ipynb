{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNP7nRp0dnwACJmlS/eaA+N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jai-Kumar786/Full-Fledged-BERT-Question-Answering-Application/blob/main/04_interactive_qa_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook 4: Interactive Q&A System\n",
        "## BERT Question Answering Project\n",
        "\n",
        "**Objectives:** Load fine-tuned model, create interactive Q&A interface\n",
        "**Final Deliverable:** Production-ready question answering system\n"
      ],
      "metadata": {
        "id": "srPnbDsv8LTE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6bToeerw7txy",
        "outputId": "03b0ffbd-0028-424a-a6c1-57835f899b07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Loading fine-tuned BERT model...\n",
            "✅ Model loaded: BertForQuestionAnswering\n",
            "✅ Tokenizer loaded: BertTokenizerFast\n",
            "✅ Model parameters: 108,893,186\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# LOAD FINE-TUNED MODEL & TOKENIZER\n",
        "# ============================================\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
        "import torch\n",
        "\n",
        "print(\"📦 Loading fine-tuned BERT model...\")\n",
        "\n",
        "# Load tokenizer and model from saved directory\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./bert-qa-model\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"./bert-qa-model\")\n",
        "\n",
        "print(f\"✅ Model loaded: {model.__class__.__name__}\")\n",
        "print(f\"✅ Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
        "print(f\"✅ Model parameters: {model.num_parameters():,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Question Answering Pipeline ​\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Action: Use Hugging Face pipeline for easy inference\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "97Za356hAJ6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CREATE QA PIPELINE\n",
        "# ============================================\n",
        "\n",
        "# Create question-answering pipeline\n",
        "qa_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1  # GPU if available\n",
        ")\n",
        "\n",
        "print(\"✅ QA Pipeline created!\")\n",
        "print(f\"✅ Running on: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YERmFXJyAQdP",
        "outputId": "38e3c6bd-a91c-4fc5-8106-a9119350f08a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ QA Pipeline created!\n",
            "✅ Running on: GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# TEST THE PIPELINE\n",
        "# ============================================\n",
        "\n",
        "# Test with a simple example\n",
        "test_context = \"\"\"\n",
        "Paris is the capital and most populous city of France.\n",
        "The city has a population of 2.2 million people.\n",
        "Paris is known for the Eiffel Tower and the Louvre Museum.\n",
        "\"\"\"\n",
        "\n",
        "test_question = \"What is the capital of France?\"\n",
        "\n",
        "# Get answer\n",
        "result = qa_pipeline(question=test_question, context=test_context)\n",
        "\n",
        "print(\"\\n🔍 TEST QUERY:\")\n",
        "print(f\"Question: {test_question}\")\n",
        "print(f\"Answer: {result['answer']}\")\n",
        "print(f\"Confidence: {result['score']:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tbW3eYlHArxl",
        "outputId": "933bf0c3-d63f-4ca3-8555-44765d5cab31"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 TEST QUERY:\n",
            "Question: What is the capital of France?\n",
            "Answer: Paris\n",
            "Confidence: 67.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4.2: Build interactive Q&A system (3 marks)"
      ],
      "metadata": {
        "id": "WPLsr1_XA8NZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# QUESTION 4.2: Interactive Q&A System (3 marks)\n",
        "# ============================================\n",
        "\n",
        "def interactive_qa():\n",
        "    \"\"\"\n",
        "    Interactive Q&A system using fine-tuned BERT model.\n",
        "    User provides context and question, system returns answer.\n",
        "    Type 'quit' to exit.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"🤖 BERT Question Answering System\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Instructions:\")\n",
        "    print(\"  1. Enter a context (paragraph of text)\")\n",
        "    print(\"  2. Enter a question about the context\")\n",
        "    print(\"  3. Get an answer from BERT!\")\n",
        "    print(\"  - Type 'quit' at any prompt to exit\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    while True:\n",
        "        # Get context from user\n",
        "        print(\"\\n📝 Enter Context:\")\n",
        "        context = input(\"> \")\n",
        "\n",
        "        # Check for quit\n",
        "        if context.lower() == 'quit':\n",
        "            print(\"\\n👋 Thanks for using BERT QA! Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Validate context\n",
        "        if len(context.strip()) < 10:\n",
        "            print(\"⚠️ Context too short! Please provide at least 10 characters.\")\n",
        "            continue\n",
        "\n",
        "        # Get question from user\n",
        "        print(\"\\n❓ Enter Question:\")\n",
        "        question = input(\"> \")\n",
        "\n",
        "        # Check for quit\n",
        "        if question.lower() == 'quit':\n",
        "            print(\"\\n👋 Thanks for using BERT QA! Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Validate question\n",
        "        if len(question.strip()) < 3:\n",
        "            print(\"⚠️ Question too short! Please ask a proper question.\")\n",
        "            continue\n",
        "\n",
        "        # Get answer from model\n",
        "        print(\"\\n🔍 Processing...\")\n",
        "        try:\n",
        "            result = qa_pipeline(question=question, context=context)\n",
        "\n",
        "            # Display results\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"✅ ANSWER:\")\n",
        "            print(f\"   {result['answer']}\")\n",
        "            print(f\"\\n📊 Confidence: {result['score']:.2%}\")\n",
        "            print(f\"📍 Position: characters {result['start']}-{result['end']}\")\n",
        "            print(\"=\"*70)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ Error: {str(e)}\")\n",
        "            print(\"Please try again with different input.\")\n",
        "\n",
        "# Start the interactive system\n",
        "interactive_qa()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SR_ElsnqA_QZ",
        "outputId": "df01cce0-ac88-4994-aeb1-1433678ba923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "🤖 BERT Question Answering System\n",
            "======================================================================\n",
            "Instructions:\n",
            "  1. Enter a context (paragraph of text)\n",
            "  2. Enter a question about the context\n",
            "  3. Get an answer from BERT!\n",
            "  - Type 'quit' at any prompt to exit\n",
            "======================================================================\n",
            "\n",
            "\n",
            "📝 Enter Context:\n",
            "> The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel.\n",
            "\n",
            "❓ Enter Question:\n",
            "> Who is the Eiffel Tower named after?\n",
            "\n",
            "🔍 Processing...\n",
            "\n",
            "======================================================================\n",
            "✅ ANSWER:\n",
            "   Gustave Eiffel\n",
            "\n",
            "📊 Confidence: 76.41%\n",
            "📍 Position: characters 119-133\n",
            "======================================================================\n",
            "\n",
            "📝 Enter Context:\n"
          ]
        }
      ]
    }
  ]
}